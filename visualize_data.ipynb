{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathyakrishnansuresh/Desktop/gpt-k/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import TranslationDataset, DataLoader\n",
    "import config\n",
    "from tokenizer import Tokenizer\n",
    "import torch\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model_path=f\"{config.llama_path}/tokenizer.model\")\n",
    "ds = TranslationDataset(\n",
    "    dataset_hf_id=\"de-en\",\n",
    "    source_lang=\"de\",\n",
    "    target_lang=\"en\",\n",
    "    split=\"validation\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "dl = DataLoader(\n",
    "    dataset=ds,\n",
    "    batch_size=32,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non trainable part: <|begin_of_text|>Translate: De: Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten En: \n",
      "Trainable part: A Republican strategy to counter the re-election of Obama<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "op = ds[0]\n",
    "print(f\"Non trainable part: {tokenizer.decode(op[0][:op[1]])}\")\n",
    "print(f\"Trainable part: {tokenizer.decode(op[0][op[1]:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non trainable part: [128000, 28573, 25, 220, 1951, 25, 52410, 107684, 276, 10782, 29323, 648, 11, 4543, 2761, 468, 22970, 73065, 6675, 7250, 1218, 713, 4469, 89, 332, 86985, 2998, 25, 220]\n",
      "Trainable part: [32, 9540, 8446, 311, 5663, 279, 312, 43733, 315, 7250, 128001]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Non trainable part: {op[0][:op[1]]}\")\n",
    "print(f\"Trainable part: {op[0][op[1]:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 1st token prediction:\n",
      "input: [220, 32, 9540, 8446, 311, 5663, 279, 312, 43733, 315, 7250]\n",
      "target: [32, 9540, 8446, 311, 5663, 279, 312, 43733, 315, 7250, 128001]\n",
      "for 2nd token prediction:\n",
      "input: [220, 32, 9540, 8446, 311, 5663, 279, 312, 43733, 315]\n",
      "target: [9540, 8446, 311, 5663, 279, 312, 43733, 315, 7250, 128001]\n",
      "for 3rd token prediction:\n",
      "input: [220, 32, 9540, 8446, 311, 5663, 279, 312, 43733]\n",
      "target: [8446, 311, 5663, 279, 312, 43733, 315, 7250, 128001]\n"
     ]
    }
   ],
   "source": [
    "# 1st token\n",
    "print(f\"for 1st token prediction:\")\n",
    "inp, tgt = op[0][op[1]-1:-1], op[0][op[1]:]\n",
    "assert len(inp) == len(tgt)\n",
    "assert inp[1:] == tgt[:-1]\n",
    "print(f\"input: {inp}\")\n",
    "print(f\"target: {tgt}\")\n",
    "\n",
    "# 2nd token\n",
    "print(f\"for 2nd token prediction:\")\n",
    "inp, tgt = op[0][op[1]-1:-2], op[0][op[1]+1:]\n",
    "assert len(inp) == len(tgt)\n",
    "assert inp[2:] == tgt[:-2]\n",
    "print(f\"input: {inp}\")\n",
    "print(f\"target: {tgt}\")\n",
    "\n",
    "print(f\"for 3rd token prediction:\")\n",
    "inp, tgt = op[0][op[1]-1:-3], op[0][op[1]+2:]\n",
    "assert len(inp) == len(tgt)\n",
    "assert inp[3:] == tgt[:-3]\n",
    "print(f\"input: {inp}\")\n",
    "print(f\"target: {tgt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, start_positions = ds[2:2+10]\n",
    "len(tokens), len(start_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 133]) torch.Size([32, 1])\n",
      "torch.Size([32, 108]) torch.Size([32, 1])\n",
      "torch.Size([32, 155]) torch.Size([32, 1])\n",
      "torch.Size([32, 184]) torch.Size([32, 1])\n",
      "torch.Size([32, 162]) torch.Size([32, 1])\n",
      "torch.Size([32, 178]) torch.Size([32, 1])\n",
      "torch.Size([32, 136]) torch.Size([32, 1])\n",
      "torch.Size([32, 179]) torch.Size([32, 1])\n",
      "torch.Size([32, 82]) torch.Size([32, 1])\n",
      "torch.Size([32, 84]) torch.Size([32, 1])\n",
      "torch.Size([32, 127]) torch.Size([32, 1])\n",
      "torch.Size([32, 130]) torch.Size([32, 1])\n",
      "torch.Size([32, 116]) torch.Size([32, 1])\n",
      "torch.Size([32, 228]) torch.Size([32, 1])\n",
      "torch.Size([32, 131]) torch.Size([32, 1])\n",
      "torch.Size([32, 143]) torch.Size([32, 1])\n",
      "torch.Size([32, 107]) torch.Size([32, 1])\n",
      "torch.Size([32, 142]) torch.Size([32, 1])\n",
      "torch.Size([32, 136]) torch.Size([32, 1])\n",
      "torch.Size([32, 176]) torch.Size([32, 1])\n",
      "torch.Size([32, 127]) torch.Size([32, 1])\n",
      "torch.Size([32, 118]) torch.Size([32, 1])\n",
      "torch.Size([32, 187]) torch.Size([32, 1])\n",
      "torch.Size([32, 170]) torch.Size([32, 1])\n",
      "torch.Size([32, 148]) torch.Size([32, 1])\n",
      "torch.Size([32, 140]) torch.Size([32, 1])\n",
      "torch.Size([32, 179]) torch.Size([32, 1])\n",
      "torch.Size([32, 117]) torch.Size([32, 1])\n",
      "torch.Size([32, 101]) torch.Size([32, 1])\n",
      "torch.Size([32, 124]) torch.Size([32, 1])\n",
      "torch.Size([32, 99]) torch.Size([32, 1])\n",
      "torch.Size([32, 151]) torch.Size([32, 1])\n",
      "torch.Size([32, 212]) torch.Size([32, 1])\n",
      "torch.Size([32, 121]) torch.Size([32, 1])\n",
      "torch.Size([32, 131]) torch.Size([32, 1])\n",
      "torch.Size([32, 107]) torch.Size([32, 1])\n",
      "torch.Size([32, 127]) torch.Size([32, 1])\n",
      "torch.Size([32, 156]) torch.Size([32, 1])\n",
      "torch.Size([32, 132]) torch.Size([32, 1])\n",
      "torch.Size([32, 258]) torch.Size([32, 1])\n",
      "torch.Size([32, 260]) torch.Size([32, 1])\n",
      "torch.Size([32, 286]) torch.Size([32, 1])\n",
      "torch.Size([32, 168]) torch.Size([32, 1])\n",
      "torch.Size([32, 187]) torch.Size([32, 1])\n",
      "torch.Size([32, 162]) torch.Size([32, 1])\n",
      "torch.Size([32, 128]) torch.Size([32, 1])\n",
      "torch.Size([32, 158]) torch.Size([32, 1])\n",
      "torch.Size([32, 167]) torch.Size([32, 1])\n",
      "torch.Size([32, 122]) torch.Size([32, 1])\n",
      "torch.Size([32, 97]) torch.Size([32, 1])\n",
      "torch.Size([32, 108]) torch.Size([32, 1])\n",
      "torch.Size([32, 117]) torch.Size([32, 1])\n",
      "torch.Size([32, 98]) torch.Size([32, 1])\n",
      "torch.Size([32, 149]) torch.Size([32, 1])\n",
      "torch.Size([32, 137]) torch.Size([32, 1])\n",
      "torch.Size([32, 156]) torch.Size([32, 1])\n",
      "torch.Size([32, 147]) torch.Size([32, 1])\n",
      "torch.Size([32, 92]) torch.Size([32, 1])\n",
      "torch.Size([32, 97]) torch.Size([32, 1])\n",
      "torch.Size([32, 140]) torch.Size([32, 1])\n",
      "torch.Size([32, 111]) torch.Size([32, 1])\n",
      "torch.Size([32, 128]) torch.Size([32, 1])\n",
      "torch.Size([32, 115]) torch.Size([32, 1])\n",
      "torch.Size([32, 125]) torch.Size([32, 1])\n",
      "torch.Size([32, 126]) torch.Size([32, 1])\n",
      "torch.Size([32, 155]) torch.Size([32, 1])\n",
      "torch.Size([32, 132]) torch.Size([32, 1])\n",
      "torch.Size([32, 158]) torch.Size([32, 1])\n",
      "torch.Size([32, 146]) torch.Size([32, 1])\n",
      "torch.Size([32, 195]) torch.Size([32, 1])\n",
      "torch.Size([32, 165]) torch.Size([32, 1])\n",
      "torch.Size([32, 181]) torch.Size([32, 1])\n",
      "torch.Size([32, 180]) torch.Size([32, 1])\n",
      "torch.Size([32, 134]) torch.Size([32, 1])\n",
      "torch.Size([32, 159]) torch.Size([32, 1])\n",
      "torch.Size([32, 189]) torch.Size([32, 1])\n",
      "torch.Size([32, 104]) torch.Size([32, 1])\n",
      "torch.Size([32, 97]) torch.Size([32, 1])\n",
      "torch.Size([32, 264]) torch.Size([32, 1])\n",
      "torch.Size([32, 143]) torch.Size([32, 1])\n",
      "torch.Size([32, 215]) torch.Size([32, 1])\n",
      "torch.Size([32, 113]) torch.Size([32, 1])\n",
      "torch.Size([32, 130]) torch.Size([32, 1])\n",
      "torch.Size([32, 124]) torch.Size([32, 1])\n",
      "torch.Size([32, 183]) torch.Size([32, 1])\n",
      "torch.Size([32, 175]) torch.Size([32, 1])\n",
      "torch.Size([32, 204]) torch.Size([32, 1])\n",
      "torch.Size([32, 246]) torch.Size([32, 1])\n",
      "torch.Size([32, 228]) torch.Size([32, 1])\n",
      "torch.Size([32, 192]) torch.Size([32, 1])\n",
      "torch.Size([32, 189]) torch.Size([32, 1])\n",
      "torch.Size([32, 146]) torch.Size([32, 1])\n",
      "torch.Size([32, 165]) torch.Size([32, 1])\n",
      "torch.Size([24, 159]) torch.Size([24, 1])\n"
     ]
    }
   ],
   "source": [
    "for tokens, start_positions in dl:\n",
    "    print(tokens.shape, start_positions.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
